#!/usr/bin/env python3
"""
Incrementally compute attribute values for all available assemblies.

Usage:
  snakemake -j -s prostdb_attrs.snake

Setup:
  - define the list of plugins and input sources to be used, in the
    `plugins_datasrc` list in the "all" rule
  - if the input source for a plugin is not the locally stored data
    (fna, gff...) then a new "compute_..." rule must be defined
    (e.g. if data is downloaded from an external database)

Effects:
  - computes not-yet computed attribute values for all assemblies
    using the batch_compute.py (in parallel) and the specified plugins
  - the results of the computation is first stored to file
    (under path.attrs, which by default is <prostdatadir>/assembly_attributes)
  - then the computation results and the attributes, plugin and computation
    metadata are loaded into ProstDB

Implementation:
  - "all" requires "compute_and_load"; the requirements of this rule are
    computed dynamically (input_of_compute_and_load function) and the rule
    is executed twice, first in the "compute" phase and (since this is a
    checkpoint which forces the recomputation of the prerequisites) a second
    time in the "load" phase.
  - the "compute" phase (computing the attribute values for the assemblies for
    which the values were not computed yet) consists of checkpoint rules,
    such as the "compute_on_local_refseq_files" rule
  - the "load" phase (load the results and metadata of the computation into the
    database) first makes sure that the database is prepared for storing the
    metadata (create_attributes rule -> create_all_tables -> create_tables);
    then it loads the attribute values and the metadata into the database
    (process_loaded -> load_results)

"""

import os

include: "common.snake"

rule all:
  input:
    expand(str(path.attrs/"done.compute_and_load.{plugin_datasrc}.{domain}"),
        plugin_datasrc=["fas_stats_posix.refseq_genomic_fna",
                        "refseq_ftypes.refseq_genomic_gff"],
        domain=domains)

rule create_all_tables:
  input:
    dbsocket=ancient(config['dbsocket']),
  output:
    done=touch(path.flags/"done.create_tables.attrs")
  shell:
    """
    prenacs-setup-database {config[dbuser] {config[dbpass]} \
        {config[dbname]} {config[dbsocket]}
    """

rule create_attributes:
  input:
    dbsocket=ancient(config['dbsocket']),
    definitions=srcdir("../plugins/attributes.yaml"),
    done=rules.create_all_tables.output.done
  params:
    update=True,
    drop=True
  output:
    done=touch(path.flags/"done.create_attributes")
  shell:
    """
    prenacs-manage-attributes {config[dbuser]} {config[dbpass]} \
        {config[dbname]} {config[dbsocket]} {input.definitions} --drop --update
    """

def plugin_module_path(w):
  pfx = os.path.join(workflow.basedir, f"../plugins/{w.plugin}")
  result = pfx+".py" if os.path.exists(pfx+".py") else pfx+".nim"
  return ancient(result)

def skipopt_value(w):
  skipfile = path.attrs/f"{w.plugin}.{w.datasrc}.{w.domain}.tsv"
  return skipfile if os.path.exists(skipfile) else ""

ext_for_datasrc = {
    "refseq_genomic_fna": "genomic.fna.gz",
    "refseq_genomic_gff": "genomic.gff.gz"
  }

def globpattern_value(w):
  return str(path.genomes / w.datasrc / w.domain / "chunk.*" / \
             f"*_{ext_for_datasrc[w.datasrc]}")

checkpoint compute_on_local_refseq_files:
  input:
    plugin=plugin_module_path,
    ts=path.asmsummary/"assembly_summary_refseq_{domain}.update_timestamp",
    idsproc=ancient(\
        srcdir("../plugins/refseq_accession_from_filename.py"))
  output:
    report=str(path.attrs/"{plugin}.{datasrc}.{domain}.report.yaml"),
    out=str(path.attrs/"{plugin}.{datasrc}.{domain}.new_data.tsv"),
    log=str(path.attrs/"{plugin}.{datasrc}.{domain}.log")
  params:
    globpattern=globpattern_value,
    reason="new_entities",
    skip=skipopt_value,
    verbose=True,
    serial=False
  shell:
    """
    if [ "{params.skip}" == "" ]; then SKIPOPT=""; \
    else SKIPOPT="--skip {params.skip}"; fi
    prenacs-batch-compute {input.plugin} files "{params.globpattern}" \
        --idsproc {input.idsproc} --verbose $SKIPOPT \
        --out {output.out} --log {output.log} --reason new_entities \
        --report {output.report}
    """

rule load_results:
  input:
    results=path.attrs/"{plugin}.{datasrc}.{domain}.new_data.tsv",
    report=path.attrs/"{plugin}.{datasrc}.{domain}.report.yaml",
    plugin=plugin_module_path,
    dbsocket=ancient(config['dbsocket']),
    done=rules.create_attributes.output.done
  output:
    touch(path.attrs/"done.load.{plugin}.{datasrc}.{domain}")
  shell:
    """
    prenacs-load-results {config[dbuser]} {config[dbpass]} {config[dbname]} \
        {config[dbsocket]} {input.results} {input.report} {input.plugin}
    """

rule process_loaded:
  input:
    new=path.attrs/"{plugin}.{datasrc}.{domain}.new_data.tsv",
    done=path.attrs/"done.load.{plugin}.{datasrc}.{domain}",
    report=path.attrs/"{plugin}.{datasrc}.{domain}.report.yaml"
  output:
    touch(path.attrs/"done.process_loaded.{plugin}.{datasrc}.{domain}")
  params:
    out=str(path.attrs/"{plugin}.{datasrc}.{domain}.tsv")
  shell:
    """
    echo "Number of existing data rows:"
    if [ -e {params.out} ]; then
      wc -l {params.out}
    else
      echo "0 (no file with existing data)"
    fi
    echo "Number of new data rows:"
    wc -l {input.new}
    echo "Appending new data from {input.new} to {params.out}..."
    cat {input.new} >> {params.out}
    echo "Removing {input.new}..."
    rm {input.new}
    """

def input_of_compute_and_load_rule(w):
  compute=str(path.attrs/f"{w.plugin}.{w.datasrc}.{w.domain}.report.yaml")
  load=str(path.attrs/f"done.process_loaded.{w.plugin}.{w.datasrc}.{w.domain}")
  result = [compute]
  if os.path.exists(compute):
    result.append(load)
  return result

rule compute_and_load:
  input: input_of_compute_and_load_rule
  output:
    touch(path.attrs/"done.compute_and_load.{plugin}.{datasrc}.{domain}")

